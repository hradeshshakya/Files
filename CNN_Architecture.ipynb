{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a Convolutional Neural Network (CNN), and why is it used for image processing?**\n",
        "\n",
        "A CNN is a type of artificial neural network specifically designed for processing grid-like data, such as images. They are inspired by the visual cortex of animals and excel at detecting patterns and features within images.\n",
        "\n",
        "CNNs are exceptionally well-suited for image processing due to the following reasons:\n",
        "\n",
        "Feature Extraction: CNNs automatically learn relevant features from images through a process called convolution. This eliminates the need for manual feature engineering, which can be time-consuming and less effective.\n",
        "\n",
        "Spatial Hierarchy: CNNs leverage the spatial relationships between pixels in an image by using filters (kernels) that slide across the image. This hierarchical approach allows CNNs to recognize complex patterns by combining simpler features.\n",
        "\n",
        "Translation Invariance: CNNs are relatively robust to changes in the position of objects within an image. This property, known as translation invariance, is crucial for object recognition and image classification tasks.\n",
        "\n",
        "Parameter Sharing: CNNs reduce the number of parameters compared to fully connected networks by sharing weights across different parts of the image. This makes CNNs more efficient and less prone to overfitting.\n",
        "\n",
        "\n",
        "**2.  What are the key components of a CNN architecture?**\n",
        "\n",
        "Input Layer: This layer receives the raw image data as input. Images are represented as multi-dimensional arrays (e.g., a 2D array for grayscale images and a 3D array for color images).\n",
        "\n",
        "Convolutional Layers: These layers are the core building blocks of a CNN. They apply filters (kernels) to the input image to extract features. Each filter is a small matrix of weights that is convolved with the input image to produce a feature map. The filters learn to detect specific patterns or features in the image, such as edges, corners, or textures.\n",
        "\n",
        "Activation Function: After the convolution operation, an activation function is applied to the feature maps. The activation function introduces non-linearity into the network, which is essential for learning complex patterns. Common activation functions used in CNNs include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
        "\n",
        "Pooling Layers: These layers reduce the spatial dimensions of the feature maps, reducing the number of parameters and computational complexity. Common pooling operations include max pooling and average pooling. Max pooling selects the maximum value within a pooling window, while average pooling calculates the average value.\n",
        "\n",
        "Fully Connected Layers: These layers are typically located towards the end of the CNN architecture. They take the flattened output from the previous layers and connect them to a set of output nodes. The fully connected layers learn to combine the extracted features to make predictions, such as classifying the image into different categories.\n",
        "\n",
        "Output Layer: This layer produces the final output of the CNN. The type of output layer depends on the specific task. For image classification, the output layer might consist of a softmax function that assigns probabilities to different classes. For object detection, the output layer might provide bounding boxes and class labels for objects in the image.\n",
        "\n",
        "\n",
        "**3.  What is the role of the convolutional layer in CNNs?**\n",
        "\n",
        "The convolutional layer is the fundamental building block of a CNN. It plays a crucial role in extracting features from the input image.\n",
        "\n",
        "**4. What is a filter (kernel) in CNNs?**\n",
        "\n",
        "The primary role of a filter is to extract features from the input image. It does this by performing a mathematical operation called convolution.Kernel, is a small matrix of weights. It's a crucial component of the convolutional layer, which is the core building block of a CNN.\n",
        "\n",
        "**5. What is pooling in CNNs, and why is it important?**\n",
        "\n",
        "Pooling is a downsampling operation that reduces the spatial dimensions (width and height) of the feature maps generated by the convolutional layers in a CNN. It's typically applied after the convolutional layers and before the fully connected layers.\n",
        "\n",
        "**6.  What are the common types of pooling used in CNNs?**\n",
        "\n",
        "There are several types of pooling operations used in CNNs, each with its own characteristics and benefits. Here are the most common ones:\n",
        "1.Max Pooling\n",
        "2.Average Pooling\n",
        "3.Global Average Pooling\n",
        "4.Global Max Pooling\n",
        "5.Stochastic Pooling\n",
        "\n",
        "\n",
        "**7.  How does the backpropagation algorithm work in CNNs?**\n",
        "\n",
        "Backpropagation in CNNs is a process of iteratively adjusting the network's weights based on the error it makes in its predictions. By repeatedly performing the forward pass, loss calculation, backpropagation, and weight update steps, the network gradually learns to extract relevant features from the input data and make accurate predictions.\n",
        "\n",
        "**8.  What is the role of activation functions in CNNs?**\n",
        "\n",
        "Activation functions are essential components of CNNs, introducing non-linearity, enabling decision boundaries, aiding feature extraction, and supporting gradient-based learning. Choosing the appropriate activation function can significantly impact the network's performance and ability to learn complex patterns.\n",
        "\n",
        "**9. What is the concept of receptive fields in CNNs?*\n",
        "\n",
        "Receptive Field: The Window of Perception\n",
        "\n",
        "In Convolutional Neural Networks (CNNs), the receptive field of a neuron refers to the region of the input image that influences the neuron's activation. It's essentially the neuron's \"window of perception\" within the input image.\n",
        "\n",
        "**10.  Explain the concept of tensor space in CNNs.**\n",
        "\n",
        "Tensor space is a fundamental concept in CNNs, providing a framework for representing data and features as tensors. It enables CNNs to capture spatial relationships, extract hierarchical features, and perform complex mathematical operations. By understanding tensor space, we gain a deeper understanding of how CNNs process and learn from image data.\n",
        "\n",
        "**11.  What is LeNet-5, and how does it contribute to the development of CNNs?**\n",
        "\n",
        "LeNet-5, developed by Yann LeCun and his colleagues in the 1990s, is one of the earliest and most influential Convolutional Neural Networks (CNNs). It was designed for handwritten digit recognition and achieved remarkable success, paving the way for the advancement of deep learning in computer vision.\n",
        "\n",
        "LeNet-5 made significant contributions to the development of CNNs in several ways:\n",
        "\n",
        "Early Success: Its success in handwritten digit recognition demonstrated the potential of CNNs for real-world applications, sparking further research and development in the field.\n",
        "\n",
        "Architectural Blueprint: LeNet-5's architecture, with its convolutional, pooling, and fully connected layers, became a blueprint for many subsequent CNN architectures.\n",
        "\n",
        "Backpropagation: LeNet-5 utilized the backpropagation algorithm for training, which is now a standard technique for training neural networks.\n",
        "\n",
        "Feature Extraction: It showcased the power of CNNs to automatically learn and extract relevant features from image data, eliminating the need for manual feature engineering.\n",
        "\n",
        "**12.  What is AlexNet, and why was it a breakthrough in deep learning?**\n",
        "\n",
        "AlexNet: A Deep Learning Revolution\n",
        "\n",
        "AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, is a deep Convolutional Neural Network (CNN) that made a significant breakthrough in the field of deep learning and computer vision. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a large margin, demonstrating the power of deep learning for image classification.\n",
        "\n",
        "Breakthrough in Deep Learning:\n",
        "\n",
        "AlexNet's victory in the ILSVRC 2012 competition marked a turning point in deep learning for several reasons:\n",
        "\n",
        "Superior Performance: It achieved a top-5 error rate of 15.3%, significantly outperforming the second-best entry with an error rate of 26.2%. This demonstrated the remarkable accuracy and potential of deep learning for image classification.\n",
        "\n",
        "Revival of Deep Learning: AlexNet's success revitalized interest in deep learning, which had been largely dormant for several years. It sparked a renewed focus on deep neural networks and paved the way for further advancements in the field.\n",
        "\n",
        "Influence on Subsequent Architectures: AlexNet's architecture and innovations, such as ReLU, dropout, and data augmentation, became standard practices in deep learning and heavily influenced the design of subsequent CNN architectures.\n",
        "\n",
        "**13.  What is VGGNet, and how does it differ from AlexNet?**\n",
        "VGGNet, developed by the Visual Geometry Group (VGG) at the University of Oxford, is a deep Convolutional Neural Network (CNN) architecture known for its simplicity and depth. It achieved impressive results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014, further demonstrating the power of deep learning for image classification.\n",
        "\n",
        "Key Features and Differences from AlexNet:\n",
        "\n",
        "Increased Depth: VGGNet is significantly deeper than AlexNet, with variations containing 16 or 19 layers. This increased depth allows the network to learn more complex and abstract features from images.\n",
        "\n",
        "Smaller Filters: VGGNet primarily uses small 3x3 convolutional filters throughout the network, compared to AlexNet's larger filters (11x11 and 5x5 in the first two layers). This use of smaller filters reduces the number of parameters and makes the network more efficient.\n",
        "\n",
        "Increased Number of Filters: While using smaller filters, VGGNet increases the number of filters in each layer, compensating for the reduced receptive field size of individual filters and maintaining the network's capacity to learn complex features.\n",
        "\n",
        "Simplicity: VGGNet has a relatively simple and uniform architecture, with convolutional layers stacked on top of each other and followed by pooling layers. This simplicity makes the network easier to understand and implement.\n",
        "\n",
        "Improved Performance: VGGNet achieved state-of-the-art results in the ILSVRC 2014 competition, demonstrating the effectiveness of its deeper and simpler architecture.\n",
        "\n",
        "**14.  What is GoogLeNet, and what is its main innovation?**\n",
        "\n",
        "GoogLeNet, also known as Inception v1, is a deep Convolutional Neural Network (CNN) architecture developed by Google researchers. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014 and introduced a novel architectural innovation called the Inception module.\n",
        "\n",
        "The main innovation of GoogLeNet is the Inception module, which aims to improve the efficiency and performance of deep neural networks. Instead of simply stacking convolutional layers sequentially, the Inception module allows the network to perform multiple convolutions and pooling operations in parallel and concatenate their outputs.\n",
        "\n",
        "**15. What is ResNet, and what problem does it solve?**\n",
        "\n",
        "ResNet, short for Residual Network, is a deep Convolutional Neural Network (CNN) architecture that was introduced to address the vanishing gradient problem, a major challenge in training very deep neural networks.\n",
        "\n",
        "**16.  What is DenseNet, and how does it differ from ResNet?**\n",
        "\n",
        "DenseNet, short for Densely Connected Convolutional Network, is a deep learning architecture that further extends the idea of skip connections introduced in ResNet. It aims to improve information flow and feature reuse within the network by connecting each layer to every other layer in a feed-forward fashion.\n",
        "\n",
        "Key Differences from ResNet:\n",
        "\n",
        "Dense Connectivity: In ResNet, skip connections bypass one or more layers, whereas in DenseNet, each layer is directly connected to all preceding layers. This dense connectivity creates a \"highway\" of information flow throughout the network.\n",
        "\n",
        "Feature Reuse: DenseNet encourages feature reuse by allowing each layer to access the feature maps of all preceding layers. This helps the network learn more complex and discriminative features.\n",
        "\n",
        "Reduced Parameters: Despite the dense connections, DenseNet can have fewer parameters than ResNet due to the efficient feature reuse and reduced need for wide layers.\n",
        "\n",
        "Improved Performance: DenseNet has achieved state-of-the-art results on various image recognition tasks, demonstrating the effectiveness of its dense connectivity and feature reuse.\n",
        "\n",
        "\n",
        "**17.  What are the main steps involved in training a CNN from scratch?**\n",
        "\n",
        "Okay, let's outline the main steps involved in training a CNN from scratch:\n",
        "\n",
        "1. Data Preparation:\n",
        "\n",
        "Gather and Preprocess Data: Collect a large and diverse dataset of images relevant to your task. Preprocess the images by resizing, normalizing pixel values, and potentially augmenting the data (e.g., rotations, flips) to increase its size and variability.\n",
        "Split Data: Divide the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set is used to evaluate the final model's performance on unseen data.\n",
        "2. Model Design:\n",
        "\n",
        "Choose Architecture: Select a suitable CNN architecture based on your task and computational resources. Consider factors like depth, filter sizes, number of layers, and activation functions. You can start with a simpler architecture like LeNet or AlexNet and then experiment with more complex ones like VGGNet, GoogLeNet, or ResNet.\n",
        "Define Layers: Define the layers of your CNN, including convolutional layers, pooling layers, activation functions, and fully connected layers. Specify the number of filters, kernel sizes, strides, padding, and other parameters for each layer.\n",
        "3. Model Compilation:\n",
        "\n",
        "Choose Optimizer: Select an optimization algorithm to update the model's weights during training. Popular choices include Stochastic Gradient Descent (SGD), Adam, and RMSprop.\n",
        "Define Loss Function: Choose a loss function that measures the difference between the model's predictions and the actual targets. Common loss functions for image classification include categorical cross-entropy and sparse categorical cross-entropy.\n",
        "Set Metrics: Define metrics to track the model's performance during training and evaluation. Accuracy, precision, recall, and F1-score are commonly used metrics for image classification.\n",
        "4. Model Training:\n",
        "\n",
        "Feed Data: Feed the training data to the model in batches.\n",
        "Forward Pass: The model makes predictions on the input data.\n",
        "Loss Calculation: The loss function is used to calculate the error between the predictions and the actual targets.\n",
        "Backpropagation: The gradients of the loss function with respect to the model's weights are calculated and propagated back through the network.\n",
        "Weight Update: The optimizer uses the gradients to update the model's weights, aiming to minimize the loss function.\n",
        "Repeat: This process is repeated for multiple epochs (iterations over the entire training dataset) until the model converges and achieves satisfactory performance on the validation set.\n",
        "5. Model Evaluation and Fine-tuning:\n",
        "\n",
        "Evaluate on Test Set: After training, evaluate the model's performance on the test set to get an unbiased estimate of its generalization ability.\n",
        "Fine-tune Hyperparameters: If the model's performance is not satisfactory, adjust the hyperparameters (e.g., learning rate, batch size, number of epochs) and retrain the model.\n",
        "Iterate: Repeat the training and evaluation process until the desired performance is achieved.\n",
        "6. Model Deployment:\n",
        "\n",
        "Once the model is trained and evaluated, it can be deployed for inference on new, unseen data. This might involve saving the model's weights and using them to make predictions in a separate application or environment.\n"
      ],
      "metadata": {
        "id": "yi88XWgCAUFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "xbIltu4LmuoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **1. Implement a basic convolution operation using a filter and a 5x5 image (matrix).**"
      ],
      "metadata": {
        "id": "uy5rXi_wm0xp"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def convolve(image, kernel):\n",
        "  \"\"\"Applies a 3x3 convolution filter to a 5x5 image.\n",
        "\n",
        "  Args:\n",
        "    image: A 5x5 NumPy array representing the image.\n",
        "    kernel: A 3x3 NumPy array representing the convolution filter.\n",
        "\n",
        "  Returns:\n",
        "    A 3x3 NumPy array representing the convolved output.\n",
        "  \"\"\"\n",
        "\n",
        "  image_height, image_width = image.shape\n",
        "  kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "  output_height = image_height - kernel_height + 1\n",
        "  output_width = image_width - kernel_width + 1\n",
        "\n",
        "  output = np.zeros((output_height, output_width))\n",
        "\n",
        "  for i in range(output_height):\n",
        "    for j in range(output_width):\n",
        "      region = image[i:i + kernel_height, j:j + kernel_width]\n",
        "      output[i, j] = np.sum(region * kernel)\n",
        "\n",
        "  return output\n",
        "\n",
        "# Example usage:\n",
        "image = np.array([\n",
        "  [1, 2, 3, 4, 5],\n",
        "  [6, 7, 8, 9, 10],\n",
        "  [11, 12, 13, 14, 15],\n",
        "  [16, 17, 18, 19, 20],\n",
        "  [21, 22, 23, 24, 25]\n",
        "])\n",
        "\n",
        "kernel = np.array([\n",
        "  [0, 1, 0],\n",
        "  [1, -4, 1],\n",
        "  [0, 1, 0]\n",
        "])\n",
        "\n",
        "output = convolve(image, kernel)\n",
        "print(output)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrc9ItH2nCqi",
        "outputId": "5de9a58b-c2e3-42d8-ffff-ad6fdbbd6e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Implement max pooling on a 4x4 feature map with a 2x2 window.**"
      ],
      "metadata": {
        "id": "KyuC_6nonZ6U"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def max_pooling(feature_map, pool_size, stride):\n",
        "  \"\"\"Applies max pooling to a feature map.\n",
        "\n",
        "  Args:\n",
        "    feature_map: A NumPy array representing the feature map.\n",
        "    pool_size: An integer or tuple representing the size of the pooling window.\n",
        "    stride: An integer or tuple representing the stride of the pooling operation.\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array representing the pooled output.\n",
        "  \"\"\"\n",
        "\n",
        "  feature_map_height, feature_map_width = feature_map.shape\n",
        "  pool_height, pool_width = pool_size if isinstance(pool_size, tuple) else (pool_size, pool_size)\n",
        "  stride_height, stride_width = stride if isinstance(stride, tuple) else (stride, stride)\n",
        "\n",
        "  output_height = int((feature_map_height - pool_height) / stride_height + 1)\n",
        "  output_width = int((feature_map_width - pool_width) / stride_width + 1)\n",
        "\n",
        "  output = np.zeros((output_height, output_width))\n",
        "\n",
        "  for i in range(output_height):\n",
        "    for j in range(output_width):\n",
        "      region = feature_map[i * stride_height:i * stride_height + pool_height,\n",
        "                           j * stride_width:j * stride_width + pool_width]\n",
        "      output[i, j] = np.max(region)\n",
        "\n",
        "  return output\n",
        "\n",
        "# Example usage:\n",
        "feature_map = np.array([\n",
        "  [1, 2, 3, 4],\n",
        "  [5, 6, 7, 8],\n",
        "  [9, 10, 11, 12],\n",
        "  [13, 14, 15, 16]\n",
        "])\n",
        "\n",
        "pool_size = 2\n",
        "stride = 2\n",
        "\n",
        "pooled_output = max_pooling(feature_map, pool_size, stride)\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YnEguC2pnr6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Implement the ReLU activation function on a feature map.**"
      ],
      "metadata": {
        "id": "LQSBI13pnw83"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(feature_map):\n",
        "  \"\"\"Applies the ReLU activation function to a feature map.\n",
        "\n",
        "  Args:\n",
        "    feature_map: A NumPy array representing the feature map.\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array representing the activated output.\n",
        "  \"\"\"\n",
        "\n",
        "  return np.maximum(0, feature_map)\n",
        "\n",
        "# Example usage:\n",
        "feature_map = np.array([\n",
        "  [-1, 2, -3, 4],\n",
        "  [5, -6, 7, -8],\n",
        "  [9, -10, 11, -12],\n",
        "  [13, -14, 15, -16]\n",
        "])\n",
        "\n",
        "activated_output = relu(feature_map)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PsTsrTLun7QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  Create a simple CNN model with one convolutional layer and a fully connected layer, using random data.**"
      ],
      "metadata": {
        "id": "NRbjAKsIn-DB"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Generate random data for input and output\n",
        "input_shape = (32, 32, 3)  # Example input shape (32x32 image with 3 channels)\n",
        "num_classes = 10  # Example number of classes for classification\n",
        "\n",
        "input_data = np.random.rand(100, *input_shape)  # 100 random input samples\n",
        "output_data = np.random.randint(0, num_classes, size=(100,))  # 100 random output labels\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, output_data, epochs=10)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "iQASuKsEoIzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Generate a synthetic dataset using random noise and train a simple CNN model on it**"
      ],
      "metadata": {
        "id": "mNefhwxNoNqh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ds9mJoqoSKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "num_samples = 1000\n",
        "img_height, img_width = 32, 32\n",
        "num_classes = 2  # Example: binary classification\n",
        "\n",
        "# Generate random noise images\n",
        "images = np.random.rand(num_samples, img_height, img_width, 1)  # Grayscale images\n",
        "\n",
        "# Assign random labels (0 or 1)\n",
        "labels = np.random.randint(0, num_classes, size=num_samples)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {accuracy}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BrXoYPWwoXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.  Create a simple CNN using Keras with one convolution layer and a max-pooling layer.**"
      ],
      "metadata": {
        "id": "I9Yb_9EwoZ69"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usfJ0ZzCoehR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aB1-m9Aqon2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.  Write a code to add a fully connected layer after the convolution and max-pooling layers in a CNN.**"
      ],
      "metadata": {
        "id": "GS608W3_ot2n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEKwER0N9wD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),  # Flatten the output for the fully connected layer\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # Fully connected layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary (optional)\n",
        "model.summary()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "mq_Y4ni490_l",
        "outputId": "23d86247-2454-42be-a92f-7126b3c7da10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5408\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m692,352\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5408</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">692,352</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m693,962\u001b[0m (2.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">693,962</span> (2.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m693,962\u001b[0m (2.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">693,962</span> (2.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.  Write a code to add  batch normalization to a simple CNN model.**"
      ],
      "metadata": {
        "id": "asthiLcf95i6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZM_dtxN99Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the CNN model with batch normalization\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization after the convolutional layer\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary (optional)\n",
        "model.summary()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "p5bbUr8W-Q9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.  Write a code to add dropout regularization to a simple CNN model.**"
      ],
      "metadata": {
        "id": "LWk6pIn3-R-F"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the CNN model with dropout regularization\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),  # Dropout layer with a rate of 0.25\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary (optional)\n",
        "model.summary()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t65iPB71-ewm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Write a code to print the architecture of the VGG16 model in Keras?**"
      ],
      "metadata": {
        "id": "juMSW4ce-gCU"
      }
    },
    {
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the VGG16 model with pre-trained weights (optional)\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QI2crU6P-m1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.  Write a code to plot the accuracy and loss graphs after training a CNN model.**"
      ],
      "metadata": {
        "id": "BoxCmRxY-nwe"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have trained a CNN model and stored the training history in 'history'\n",
        "# e.g., history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(10, 5))  # Set figure size\n",
        "\n",
        "plt.subplot(1, 2, 1)  # Create a subplot (1 row, 2 columns, first plot)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)  # Create the second subplot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()  # Display the plots"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Lxa9-sK0-xTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.  Write a code to print the architecture of the ResNet50 model in Keras?**"
      ],
      "metadata": {
        "id": "JWKaaOrY-yzi"
      }
    },
    {
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the ResNet50 model with pre-trained weights (optional)\n",
        "model = ResNet50(weights='imagenet', include_top=True)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8KFzH_QW-5x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.  Write a code to train a basic CNN model and print the training loss and accuracy after each epoch?**"
      ],
      "metadata": {
        "id": "GHkfQUcG-614"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 3. Load and pre-process data (example using MNIST)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)  # Reshape for CNN input\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# 4. Train the model with a custom callback to print metrics\n",
        "class PrintMetrics(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
        "\n",
        "print_metrics_callback = PrintMetrics()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[print_metrics_callback])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DFLgCYC0_EVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}