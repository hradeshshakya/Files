{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is image segmentation, and why is it important?**\n",
        "\n",
        "Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images.\n",
        "\n",
        "Here are some additional points about the importance of image segmentation:\n",
        "\n",
        "It is a fundamental step in many computer vision tasks, such as object detection, image classification, and image editing.\n",
        "It can be used to improve the accuracy of these tasks by providing more detailed information about the image.\n",
        "It can be used to automate tasks that would otherwise be difficult or impossible to do manually.\n",
        "\n",
        "**2. Explain the difference between image classification, object detection, and image segmentation.**\n",
        "\n",
        "the differences between image classification, object detection, and image segmentation:\n",
        "\n",
        "Image Classification:-\n",
        "\n",
        "Goal: To assign a single label to an entire image. For example, classifying an image as containing a \"cat\" or a \"dog\".\n",
        "Output: A single label for the entire image.\n",
        "Example: Identifying if an image is a picture of a cat, dog, or bird.\n",
        "\n",
        "Object Detection:-\n",
        "\n",
        "Goal: To identify the presence and location of multiple objects within an image.\n",
        "Output: Bounding boxes around each detected object, along with a label for each object.\n",
        "Example: Detecting the presence and location of cars, pedestrians, and traffic lights in a street scene.\n",
        "\n",
        "Image Segmentation:-\n",
        "\n",
        "Goal: To partition an image into multiple segments, where each segment represents a different object or region.\n",
        "Output: A pixel-level mask for each object or region, where each pixel is assigned a label.\n",
        "Example: Identifying the boundaries of a tumor in a medical image or segmenting a road scene into different regions like roads, sidewalks, and buildings.\n",
        "\n",
        "**3. What is Mask R-CNN, and how is it different from traditional object detection models?**\n",
        "\n",
        "Mask R-CNN is a deep learning model used for instance segmentation. In simpler terms, it not only detects objects in an image but also generates a high-quality segmentation mask for each instance of an object. This means it can precisely outline the boundaries of each object at a pixel level.\n",
        "\n",
        "Traditional object detection models, like Faster R-CNN, primarily focus on identifying objects and drawing bounding boxes around them. They don't provide detailed information about the shape and boundaries of each object instance.\n",
        "\n",
        "Mask R-CNN builds upon Faster R-CNN by adding a parallel branch for predicting segmentation masks alongside the existing branch for bounding box prediction. This key addition allows Mask R-CNN to achieve instance segmentation, going beyond simple object detection.\n",
        "\n",
        "**4.  What role does the \"RoIAlign\" layer play in Mask R-CNN?**\n",
        "\n",
        "The \"RoIAlign\" (Region of Interest Align) layer plays a crucial role in Mask R-CNN by addressing a key limitation of its predecessor, Faster R-CNN, which used a method called \"RoIPool\" (Region of Interest Pooling).\n",
        "\n",
        "**5.  What are semantic, instance, and panoptic segmentation?**\n",
        "\n",
        "Semantic Segmentation\n",
        "\n",
        "Goal: To classify each pixel in an image into a predefined set of categories (e.g., car, person, road, sky). It treats multiple objects of the same class as a single entity.\n",
        "Output: A pixel-level map where each pixel is assigned a class label.\n",
        "Example: In a street scene, all pixels belonging to cars would be labeled as \"car,\" regardless of whether they are different cars.\n",
        "\n",
        "Instance Segmentation\n",
        "\n",
        "Goal: To identify and delineate individual objects of the same class within an image. It distinguishes between different instances of the same object.\n",
        "Output: A pixel-level mask for each object instance, along with its class label.\n",
        "Example: In a street scene, each car would be assigned a unique mask and label, differentiating them as separate instances.\n",
        "\n",
        "Panoptic Segmentation\n",
        "\n",
        "Goal: To combine the strengths of semantic and instance segmentation, providing a comprehensive understanding of the scene. It classifies all pixels and identifies individual object instances.\n",
        "Output: A unified pixel-level map where each pixel has both a class label (semantic) and an instance ID (instance) if it belongs to a countable object.\n",
        "Example: In a street scene, panoptic segmentation would label all pixels as \"road,\" \"sky,\" \"car,\" etc. (semantic), and individual cars would have unique instance IDs.\n",
        "\n",
        "**6. Describe the role of bounding boxes and masks in image segmentation models.**\n",
        "\n",
        "Role: Bounding boxes are rectangular boxes used to locate and enclose objects within an image. They provide a rough estimate of the object's position and extent.\n",
        "\n",
        "Masks are pixel-level representations of objects within an image. They provide a detailed outline of the object's shape and boundaries.\n",
        "\n",
        "**7.  What is the purpose of data annotation in image segmentation?**\n",
        "\n",
        "Data annotation is the process of adding labels or tags to images to provide context and meaning for machine learning models. In the context of image segmentation, data annotation plays a crucial role in training and evaluating segmentation models.\n",
        "\n",
        "**8.  How does Detectron2 simplify model training for object detection and segmentation tasks?**\n",
        "\n",
        "Detectron2 simplifies model training for object detection and segmentation tasks by providing a modular and flexible framework, pre-trained models, data augmentation techniques, training utilities, visualization tools, distributed training support, and seamless integration with PyTorch.\n",
        "\n",
        "**9.  Why is transfer learning valuable in training segmentation models?**\n",
        "\n",
        "Transfer learning is valuable in training segmentation models because it improves performance, reduces training time, increases data efficiency, and enhances generalization. By leveraging the knowledge learned from pre-trained models, segmentation models can achieve better results with less data and training time.\n",
        "\n",
        "**10.  How does Mask R-CNN improve upon the Faster R-CNN model architecture?**\n",
        "\n",
        "Mask R-CNN enhances Faster R-CNN by adding the capability to generate pixel-level segmentation masks, providing a more comprehensive understanding of objects within an image. These improvements lead to higher accuracy in object delineation and enable a wider range of applications, including instance segmentation, object tracking, and medical image analysis.\n",
        "\n",
        "**11.  What is meant by \"from bounding box to polygon masks\" in image segmentation?**\n",
        "\n",
        " \"from bounding box to polygon masks\" refers to the process of refining object boundaries by converting initial bounding box annotations into detailed polygon masks.\n",
        "\n",
        "\n",
        "**12.  How does data augmentation benefit image segmentation model training?**\n",
        "\n",
        "Benefits of Data Augmentation for Image Segmentation\n",
        "\n",
        "Improved Generalization: By exposing the model to a wider range of variations in the training data, data augmentation helps the model learn more robust and generalizable features. This reduces the risk of overfitting, where the model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "Increased Data Diversity: Data augmentation introduces new and diverse training examples, simulating real-world variations in object appearance, lighting conditions, and viewpoints. This helps the model learn to handle a broader range of scenarios.\n",
        "\n",
        "Reduced Overfitting: Data augmentation effectively increases the size of the training dataset, reducing the model's tendency to memorize the training examples and improving its ability to generalize to new data.\n",
        "\n",
        "Enhanced Robustness: By introducing variations in the training data, data augmentation helps the model become more robust to noise, occlusions, and other real-world challenges.\n",
        "\n",
        "**13. Describe the architecture of Mask R-CNN, focusing on the backbone, region proposal network (RPN), and\n",
        "segmentation mask head.**\n",
        "\n",
        "Mask R-CNN Architecture: A Deep Dive\n",
        "\n",
        "Mask R-CNN is a two-stage instance segmentation model. The first stage, similar to Faster R-CNN, involves identifying regions of interest (ROIs) that potentially contain objects. The second stage then performs classification, bounding box regression, and mask prediction for each ROI.\n",
        "\n",
        "1. Backbone Network\n",
        "\n",
        "Purpose: The backbone network is responsible for extracting features from the input image. It typically uses a deep convolutional neural network (CNN) architecture, such as ResNet or ResNeXt, pre-trained on a large dataset like ImageNet.\n",
        "Function: The backbone network takes the input image and passes it through a series of convolutional and pooling layers to generate a feature map. This feature map contains rich representations of the image's content at different scales.\n",
        "Output: The output of the backbone network is a feature map that is used as input for the subsequent stages of Mask R-CNN.\n",
        "2. Region Proposal Network (RPN)\n",
        "\n",
        "Purpose: The RPN is responsible for proposing potential object regions (ROIs) within the image. It operates on the feature map generated by the backbone network.\n",
        "Function: The RPN slides a small window (called an anchor) across the feature map and predicts whether each anchor contains an object and the corresponding bounding box coordinates. It uses a set of pre-defined anchor boxes of different scales and aspect ratios to cover a wide range of object sizes.\n",
        "Output: The RPN outputs a set of region proposals (ROIs), each represented by a bounding box and an objectness score indicating the likelihood of containing an object.\n",
        "3. Segmentation Mask Head\n",
        "\n",
        "Purpose: The segmentation mask head is responsible for predicting a pixel-level segmentation mask for each ROI. It operates on the features extracted from the ROIs by the RoIAlign layer.\n",
        "Function: The mask head uses a series of convolutional layers to process the ROI features and generate a mask prediction for each object instance. The mask is a binary image where pixels belonging to the object are assigned a value of 1, and background pixels are assigned a value of 0.\n",
        "Output: The mask head outputs a segmentation mask for each ROI, providing a detailed outline of the object's shape and boundaries.\n",
        "\n",
        "**14.  Explain the process of registering a custom dataset in Detectron2 for model training.**\n",
        "\n",
        "Dataset Registration:\n",
        "\n",
        "  from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "Define the data loading function:\n",
        "\n",
        "  def my_dataset_function():\n",
        "     Load your data and annotations here\n",
        "     ...\n",
        "     Return a list of dictionaries in the expected format\n",
        "     return dataset_dicts\n",
        "\n",
        "Register the dataset:\n",
        "\n",
        "  DatasetCatalog.register(\"my_dataset_train\", my_dataset_function)\n",
        "MetadataCatalog.get(\"my_dataset_train\").set(thing_classes=[\"class1\", \"class2\", ...]) # Set class names\n",
        "\n",
        "\n",
        "**15. What challenges arise in scene understanding for image segmentation, and how can Mask R-CNN address\n",
        "them?**\n",
        "\n",
        "Challenges in Scene Understanding for Image Segmentation\n",
        "\n",
        "Scene understanding in image segmentation involves not only identifying individual objects but also understanding their relationships and context within the scene. This task poses several challenges:\n",
        "\n",
        "Object Occlusion: Objects in real-world scenes are often partially occluded by other objects, making it difficult to accurately segment them.\n",
        "\n",
        "Scale Variation: Objects can appear at different scales within an image, posing challenges for segmentation algorithms to detect and delineate objects of varying sizes.\n",
        "\n",
        "Complex Backgrounds: Real-world scenes often have cluttered and complex backgrounds, making it challenging to distinguish objects from the background.\n",
        "\n",
        "Intra-class Variation: Objects within the same class can exhibit significant variations in appearance, shape, and texture, making it difficult for segmentation models to generalize across different instances.\n",
        "\n",
        "Inter-class Similarity: Some object classes may share similar visual features, leading to confusion and misclassification during segmentation.\n",
        "\n",
        "\n",
        "How Mask R-CNN Addresses These Challenges\n",
        "\n",
        "Mask R-CNN incorporates several features that help address these challenges:\n",
        "\n",
        "Instance Segmentation: Mask R-CNN performs instance segmentation, meaning it can distinguish between individual objects of the same class, even if they are overlapping or occluded. This capability is crucial for accurate scene understanding.\n",
        "\n",
        "Feature Pyramid Network (FPN): Mask R-CNN uses FPN to extract features at multiple scales, allowing it to detect and segment objects of varying sizes. This multi-scale feature representation enhances the model's ability to handle scale variation in scenes.\n",
        "\n",
        "RoIAlign: The RoIAlign layer in Mask R-CNN ensures accurate feature extraction for mask prediction, even for small objects and objects with complex shapes. This precise feature alignment improves the model's ability to segment objects accurately, even in cluttered backgrounds.\n",
        "\n",
        "Deep Learning Architecture: Mask R-CNN leverages a deep learning architecture with powerful convolutional layers, enabling it to learn complex features and patterns from the data. This learning capability allows the model to handle intra-class variation and inter-class similarity effectively.\n",
        "\n",
        "Data Augmentation: During training, data augmentation techniques are often used to further enhance the model's robustness to variations in object appearance, lighting conditions, and viewpoints. This augmentation helps improve the model's generalization ability and performance on challenging scenes.\n",
        "\n",
        "\n",
        "**16. How is the \"IoU (Intersection over Union)\" metric used in evaluating segmentation models?**\n",
        "\n",
        "IoU: Measuring Overlap between Predicted and Ground Truth Masks\n",
        "\n",
        "The Intersection over Union (IoU), also known as the Jaccard Index, is a widely used metric for evaluating the performance of image segmentation models. It quantifies the overlap between the predicted segmentation mask and the ground truth mask for an object.\n",
        "\n",
        "\n",
        "**17.  Discuss the use of transfer learning in Mask R-CNN for improving segmentation on custom datasets.**\n",
        "\n",
        "Transfer Learning in Mask R-CNN for Custom Datasets\n",
        "\n",
        "Transfer learning is a powerful technique for improving the performance of deep learning models, especially when training data for a specific task is limited. In the context of Mask R-CNN, transfer learning involves using a model pre-trained on a large dataset (e.g., COCO) and adapting it for a custom segmentation task.\n",
        "\n",
        "**18. What is the purpose of evaluation curves, such as precision-recall curves, in segmentation model\n",
        "assessment?**\n",
        "\n",
        "Evaluation Curves: Unveiling Model Performance Trade-offs\n",
        "\n",
        "Evaluation curves, such as precision-recall curves, are graphical representations that illustrate the performance of a segmentation model across different thresholds or operating points. They provide a more comprehensive understanding of the model's behavior than single-point metrics like accuracy or IoU.\n",
        "\n",
        "Precision-Recall Curves for Segmentation\n",
        "\n",
        "In the context of segmentation, precision-recall curves are commonly used to evaluate the trade-off between precision and recall at various confidence levels.\n",
        "\n",
        "Precision: Measures the proportion of correctly predicted positive pixels (belonging to the object) among all pixels predicted as positive.\n",
        "Recall: Measures the proportion of correctly predicted positive pixels among all actual positive pixels in the ground truth.\n",
        "Purpose of Precision-Recall Curves\n",
        "\n",
        "Threshold Selection: Precision-recall curves help in selecting an appropriate threshold for the model's predictions. The threshold determines the confidence level above which a pixel is classified as belonging to the object. By analyzing the curve, we can identify the threshold that balances precision and recall according to the specific requirements of the application.\n",
        "\n",
        "Model Comparison: Precision-recall curves allow for a more nuanced comparison of different segmentation models. By comparing the curves, we can see which model performs better across different operating points and understand the trade-offs between precision and recall for each model.\n",
        "\n",
        "Performance Analysis: Precision-recall curves provide insights into the model's strengths and weaknesses. For example, a curve with high precision but low recall suggests that the model is conservative in its predictions, while a curve with high recall but low precision indicates that the model is more liberal in its predictions.\n",
        "\n",
        "Understanding Model Behavior: The shape of the precision-recall curve reveals how the model's performance changes as the threshold is varied. A steep curve indicates a rapid drop in precision as recall increases, while a flatter curve suggests a more gradual trade-off.\n",
        "\n",
        "**19.  How do Mask R-CNN models handle occlusions or overlapping objects in segmentation?**\n",
        "\n",
        "Handling Occlusions and Overlapping Objects in Mask R-CNN\n",
        "\n",
        "Mask R-CNN, by its nature of performing instance segmentation, is designed to handle occlusions and overlapping objects to a certain extent. Here's how it achieves this:\n",
        "\n",
        "Instance Segmentation: Mask R-CNN's primary strength lies in its ability to perform instance segmentation, which means it can distinguish between individual objects of the same class, even if they are overlapping or occluded. This is achieved by predicting a separate segmentation mask for each object instance, allowing the model to delineate the boundaries of each object individually.\n",
        "\n",
        "RoIAlign: The RoIAlign layer in Mask R-CNN plays a crucial role in handling occlusions. It ensures accurate feature extraction for mask prediction, even for objects that are partially obscured. By precisely aligning the extracted features with the object's region, RoIAlign helps the model to segment the visible parts of the object accurately.\n",
        "\n",
        "Feature Pyramid Network (FPN): Mask R-CNN utilizes FPN to extract features at multiple scales. This allows the model to detect and segment objects of varying sizes, which is important for handling occlusions where objects may be partially hidden behind larger objects.\n",
        "\n",
        "Non-Maximum Suppression (NMS): Mask R-CNN employs NMS to filter out redundant bounding box predictions. This helps to reduce the number of false positive detections, which can occur when objects are overlapping or occluded.\n",
        "\n",
        "Training Data: The training data used for Mask R-CNN often includes images with occlusions and overlapping objects. This exposure during training helps the model learn to recognize and handle such scenarios effectively.\n",
        "\n",
        "**20. Explain the impact of batch size and learning rate on Mask R-CNN model training.**\n",
        "\n",
        "Batch Size and Learning Rate: Key Hyperparameters in Mask R-CNN Training\n",
        "\n",
        "Batch size and learning rate are two crucial hyperparameters that significantly influence the training process and performance of Mask R-CNN models. Understanding their impact is essential for optimizing the training process and achieving optimal results.\n",
        "\n",
        "**21. Describe the challenges of training segmentation models on custom datasets, particularly in the context of\n",
        "Detectron2.**\n",
        "\n",
        "\n",
        "Challenges of Training Segmentation Models on Custom Datasets with Detectron2\n",
        "\n",
        "While Detectron2 offers a powerful and flexible framework for building and training segmentation models, several challenges can arise when working with custom datasets:\n",
        "\n",
        "Data Annotation:\n",
        "Quality and Consistency: High-quality annotations are crucial for training accurate segmentation models. Inconsistent or inaccurate annotations can lead to poor model performance and unreliable predictions. Ensuring annotation quality and consistency across the dataset is a significant challenge, especially for large and complex datasets.\n",
        "\n",
        "Annotation Format: Detectron2 supports various annotation formats, but converting custom annotations to the supported formats (e.g., COCO format) can be time-consuming and error-prone. It's essential to carefully validate the converted annotations to ensure they are correctly interpreted by Detectron2.\n",
        "\n",
        "Annotation Tooling: Selecting and using appropriate annotation tools for your specific dataset and task can be challenging. Different tools offer varying levels of functionality, ease of use, and support for different annotation types.\n",
        "\n",
        "Data Diversity and Size:\n",
        "Limited Data: Custom datasets are often smaller than publicly available datasets, which can limit the model's ability to generalize to unseen data. Addressing this challenge requires careful data augmentation and regularization techniques to prevent overfitting.\n",
        "\n",
        "Domain Adaptation: If the custom dataset significantly differs from the dataset used to pre-train the model (e.g., different object categories, image styles, or environments), domain adaptation techniques might be necessary to improve performance.\n",
        "\n",
        "Class Imbalance: Custom datasets might have an uneven distribution of object classes, leading to biased model predictions. Techniques like weighted loss functions or data augmentation can help address class imbalance issues.\n",
        "\n",
        "Model Configuration and Training:\n",
        "Hyperparameter Tuning: Finding the optimal hyperparameters for training a segmentation model on a custom dataset can be challenging. It often involves experimentation and careful monitoring of the model's performance on a validation set.\n",
        "\n",
        "Computational Resources: Training deep learning models, especially for segmentation tasks, can be computationally demanding. Access to sufficient computational resources, such as GPUs and memory, is essential for efficient training.\n",
        "\n",
        "Debugging and Evaluation: Debugging training issues and evaluating the model's performance on a custom dataset can be more complex than with standard datasets. Careful analysis of training logs, visualizations, and evaluation metrics is crucial for identifying and addressing potential problems.\n",
        "\n",
        "Detectron2-Specific Challenges:\n",
        "Dataset Registration: While Detectron2 provides a convenient way to register custom datasets, it's essential to ensure that the data loading function is correctly implemented and that the dataset is registered with the appropriate metadata.\n",
        "\n",
        "Config Settings: Properly configuring the training parameters, data augmentation settings, and model architecture within Detectron2's config system can be challenging, especially for users new to the framework.\n",
        "\n",
        "Customization: When customizing existing models or implementing new components in Detectron2, it's important to understand the framework's internal structure and API to avoid compatibility issues.\n",
        "Addressing the Challenges\n",
        "\n",
        "Careful Data Preparation: Invest time in creating high-quality annotations, using appropriate tools, and ensuring data diversity.\n",
        "\n",
        "Data Augmentation: Apply data augmentation techniques to increase the size and variability of your dataset.\n",
        "Hyperparameter Tuning: Experiment with different hyperparameter settings and use techniques like cross-validation to find the best values.\n",
        "\n",
        "Regularization: Apply regularization methods to prevent overfitting, such as dropout or weight decay.\n",
        "Transfer Learning: Leverage pre-trained models and fine-tune them on your custom dataset to improve performance and reduce training time.\n",
        "\n",
        "Domain Adaptation: If necessary, explore domain adaptation techniques to bridge the gap between the pre-training dataset and your custom dataset.\n",
        "\n",
        "Monitoring and Evaluation: Carefully monitor the training process, analyze evaluation metrics, and visualize results to identify and address potential issues.\n",
        "\n",
        "Community Support: Utilize online resources, such as the Detectron2 documentation and community forums, to seek help and guidance.\n",
        "\n",
        "**22.  How does Mask R-CNN's segmentation head output differ from a traditional object detector’s output?**\n",
        "\n",
        "Mask R-CNN vs. Traditional Object Detectors: Output Comparison\n",
        "\n",
        "The key difference between Mask R-CNN and traditional object detectors lies in their output. While traditional object detectors primarily focus on identifying objects and their bounding boxes, Mask R-CNN goes further by generating pixel-level segmentation masks for each detected object."
      ],
      "metadata": {
        "id": "lAmHsQlezEgm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2l8W2xKdEd_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "LGgkj-05EcvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.  Perform basic color-based segmentation to separate the blue color in an image.**"
      ],
      "metadata": {
        "id": "FR5an5UwEiIj"
      }
    },
    {
      "source": [
        "#Import Libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image = cv2.imread(\"image.jpg\")  # Replace \"image.jpg\" with the actual image path\n",
        "\n",
        "# Convert to HSV Color Space\n",
        "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "lower_blue = np.array([100, 50, 50])  # Lower bound for blue color in HSV\n",
        "upper_blue = np.array([130, 255, 255])  # Upper bound for blue color in HSV\n",
        "\n",
        "\n",
        "# create a mask\n",
        "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "# apply mask to the image\n",
        "result = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "# display the results\n",
        "cv2.imshow(\"Original Image\", image)\n",
        "cv2.imshow(\"Mask\", mask)\n",
        "cv2.imshow(\"Segmented Image\", result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qJPYa4KtEplt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Use edge detection with Canny to highlight object edges in an image loaded.**"
      ],
      "metadata": {
        "id": "lJiGQK0HFZ7D"
      }
    },
    {
      "source": [
        "image = cv2.imread(\"image.jpg\", cv2.IMREAD_GRAYSCALE)  # Replace \"image.jpg\" with the actual image path\n",
        "\n",
        "blurred = cv2.GaussianBlur(image, (5, 5), 0)  # Adjust kernel size as needed\n",
        "\n",
        "edges = cv2.Canny(blurred, 50, 150)  # Adjust thresholds as needed\n",
        "\n",
        "# display the results\n",
        "cv2.imshow(\"Original Image\", image)\n",
        "cv2.imshow(\"Edges\", edges)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rXuVMHBTFozy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  Load a pretrained Mask R-CNN model from PyTorch and use it for object detection and segmentation on an\n",
        "image**"
      ],
      "metadata": {
        "id": "R5niEYU7F4fS"
      }
    },
    {
      "source": [
        "# !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n",
        "\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., config/mask_rcnn_R_50_FPN_3x.yaml)\n",
        "# by in your own .yaml file\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "im = cv2.imread(\"./image.jpg\")  #load and preprocess image\n",
        "\n",
        "\n",
        "# perform inference\n",
        "outputs = predictor(im)\n",
        "\n",
        "# visulaize results\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uvOAOGzQGBLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  Generate bounding boxes for each object detected by Mask R-CNN in an image.**"
      ],
      "metadata": {
        "id": "BDyPGu-JGh0U"
      }
    },
    {
      "source": [
        "# ... (previous code for loading model and performing inference) ...\n",
        "\n",
        "# Get predicted instances\n",
        "instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "# Get bounding boxes\n",
        "boxes = instances.pred_boxes.tensor.numpy()\n",
        "\n",
        "# Visualize bounding boxes on the image\n",
        "for box in boxes:\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    cv2.rectangle(im, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw green bounding boxes\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "cv2.imshow(\"Bounding Boxes\", im)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "usvUwivIGraC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  Convert an image to grayscale and apply Otsu's thresholding method for segmentation.**"
      ],
      "metadata": {
        "id": "OYYG9ZQMGt50"
      }
    },
    {
      "source": [
        "image = cv2.imread(\"image.jpg\")  # Replace \"image.jpg\" with the actual image path\n",
        "\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
        "\n",
        "# apply Otsu's thresholding\n",
        "ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "\n",
        "# display the results\n",
        "cv2.imshow(\"Original Image\", image)\n",
        "cv2.imshow(\"Grayscale Image\", gray)\n",
        "cv2.imshow(\"Segmented Image\", thresh)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pfNKV4n-G5Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Perform contour detection in an image to detect distinct objects or shapes.**"
      ],
      "metadata": {
        "id": "F8BNb5NOHQEo"
      }
    },
    {
      "source": [
        "image = cv2.imread(\"image.jpg\")  # Replace \"image.jpg\" with the actual image path\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "edges = cv2.Canny(blurred, 50, 150)  # Adjust thresholds as needed\n",
        "\n",
        "\n",
        "# find contours\n",
        "contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)  # Draw all contours in green with thickness 2\n",
        "\n",
        "cv2.imshow(\"Contours\", image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JWU9ojAbHiyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Apply Mask R-CNN to detect objects and their segmentation masks in a custom image and display them.**"
      ],
      "metadata": {
        "id": "jRPgbo_qHuNP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ek7QJcyHzQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "#  Load Pretrained Model and Configure\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Load and Preprocess Custom Image\n",
        "im = cv2.imread(\"./custom_image.jpg\")  # Replace with your custom image path\n",
        "\n",
        "\n",
        "# perform inference\n",
        "outputs = predictor(im)\n",
        "\n",
        "# Visualize Results\n",
        "\n",
        "\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FwSBtSXRH5Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Apply k-means clustering for segmenting regions in an image.**"
      ],
      "metadata": {
        "id": "ovr-Vvw_IQxh"
      }
    },
    {
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "image = cv2.imread(\"image.jpg\")  # Replace \"image.jpg\" with the actual image path\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "pixels = image.reshape((-1, 3))  # Reshape to a list of RGB pixels\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)  # Adjust the number of clusters as needed\n",
        "kmeans.fit(pixels)\n",
        "segmented_image = kmeans.labels_.reshape(image.shape[:2])\n",
        "\n",
        "segmented_image = segmented_image.astype(np.uint8)\n",
        "# Define color map for visualizing the clusters\n",
        "color_map = np.array([[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255]])  # Example colors\n",
        "# Apply color map to the segmented image\n",
        "segmented_image_rgb = color_map[segmented_image]\n",
        "\n",
        "cv2.imshow(\"Segmented Image\", segmented_image_rgb)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "unoaj3UWIZ54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}