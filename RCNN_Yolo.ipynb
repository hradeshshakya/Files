{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.  What is the main purpose of RCNN in object detection?**\n",
        "\n",
        " RCNN's purpose is to find objects in an image, draw boxes around them, and tell you what kind of objects they are.\n",
        "\n",
        " **2.  What is the difference between Fast RCNN and Faster RCNN?**\n",
        "\n",
        " Faster R-CNN is faster and more accurate than Fast R-CNN due to the introduction of the Region Proposal Network and end-to-end training.\n",
        "Fast R-CNN is still faster than the original R-CNN but relies on an external region proposal method.\n",
        "\n",
        "**3.  How does YOLO handle object detection in real-time?**\n",
        "\n",
        " YOLO's ability to process the entire image in one pass with a single network allows it to perform object detection in real-time, making it suitable for applications like video analysis, self-driving cars, and robotics.\n",
        "\n",
        " **4 Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.**\n",
        "\n",
        " Region Proposal Networks (RPN) are a key component of Faster R-CNN that enable it to achieve real-time object detection. They replace the slower Selective Search algorithm used in previous methods, significantly speeding up the process of generating region proposals.\n",
        "\n",
        " **5. How does YOLOv9 improve upon its predecessors?**\n",
        "\n",
        "  YOLOv9 improves upon its predecessors by:\n",
        "\n",
        "Enhancing feature extraction and gradient flow with GELAN and PGI.\n",
        "Reducing computational complexity and model size with depthwise convolutions and the C3Ghost architecture.\n",
        "Achieving higher accuracy and faster inference speeds.\n",
        "These advancements make YOLOv9 a state-of-the-art object detection model, offering improved performance for a wide range of applications.\n",
        "\n",
        "**6.  What role does non-max suppression play in YOLO object detection?**\n",
        "\n",
        "Non-Maximum Suppression (NMS) is a crucial post-processing step in YOLO (and many other object detection models) that helps refine the detection results by removing redundant bounding boxes.\n",
        "\n",
        "**7. Describe the data preparation process for training YOLOv9.**\n",
        "\n",
        "Data preparation is a crucial step in training any object detection model, including YOLOv9. It involves organizing and formatting your dataset in a way that the model can understand and learn from.\n",
        "\n",
        "**8. What is the significance of anchor boxes in object detection models like YOLOv9?**\n",
        "\n",
        "While YOLOv9 is considered an \"anchor-free\" model, understanding the concept of anchor boxes is still helpful, as they played a significant role in previous object detection models and influenced the design of modern architectures.\n",
        "\n",
        "Here's the significance of anchor boxes in object detection models:\n",
        "\n",
        "Bounding Box Prediction: Anchor boxes serve as a starting point or prior for predicting the bounding boxes of objects in an image. Instead of directly predicting the coordinates of bounding boxes, the model predicts offsets or adjustments relative to these predefined anchor boxes.\n",
        "\n",
        "Handling Multiple Objects and Scales: Anchor boxes allow object detection models to handle multiple objects of different scales and aspect ratios within a single grid cell. By using a set of anchor boxes with varying sizes and shapes, the model can better capture the diversity of objects in an image.\n",
        "\n",
        "Improving Detection Accuracy: By providing a good initial guess for the bounding box location and size, anchor boxes help the model converge faster and achieve higher accuracy during training.\n",
        "\n",
        "Addressing Class Imbalance: Anchor boxes can help address the class imbalance problem in object detection, where some object classes are more prevalent than others. By assigning anchor boxes to different object classes based on their frequency, the model can better learn to detect rare objects.\n",
        "\n",
        "**9.  What is the key difference between YOLO and R-CNN architectures?**\n",
        "\n",
        "YOLO prioritizes speed and efficiency by processing the entire image in one go. It's a single-stage detector that makes predictions directly from a grid.\n",
        "\n",
        "R-CNN focuses on accuracy by using a two-stage process: first generating region proposals and then classifying objects within those regions.\n",
        "\n",
        "**10.  Why is Faster RCNN considered faster than Fast RCNN?**\n",
        "\n",
        "The primary reason for Faster R-CNN's speed advantage lies in its Region Proposal Network (RPN). This innovation significantly improves the efficiency of generating region proposals, which was a bottleneck in Fast R-CNN.\n",
        "\n",
        "**11. What is the role of selective search in RCNN?**\n",
        "\n",
        "In the context of RCNN (Regions with Convolutional Neural Networks), Selective Search plays a crucial role in generating region proposals, which are potential regions in an image that might contain objects.\n",
        "\n",
        "**12.  How does YOLOv9 handle multiple classes in object detection?**\n",
        "\n",
        " YOLOv9 handles multiple classes by predicting a probability distribution over all possible classes for each detected object. The object is then assigned to the class with the highest probability, enabling the model to detect and classify multiple objects of different classes within a single image.\n",
        "\n",
        "This approach allows YOLOv9 to perform multi-class object detection efficiently and accurately, making it a powerful tool for various real-world applications.\n",
        "\n",
        "**13.  What are the key differences between YOLOv3 and YOLOv9?**\n",
        "\n",
        "Key Improvements in YOLOv9:\n",
        "\n",
        "1.C3Ghost Architecture: A more efficient and lightweight architecture that reduces the model's size and computational cost.\n",
        "2.GELAN and PGI: Novel modules that enhance feature extraction and gradient flow, leading to improved accuracy.\n",
        "3.Anchor-Free Approach: Eliminates the need for predefined anchor boxes, simplifying the architecture and potentially improving generalization.\n",
        "4.Unified Loss Function: Combines all loss terms into a single function, improving training efficiency.\n",
        "5.Advanced Data Augmentation: Employs mosaic augmentation, mixup, and other techniques to increase data diversity and model robustness.\n",
        "6.Improved Training Strategy: Utilizes a cosine annealing scheduler and label smoothing for better optimization.\n",
        "\n",
        "**14.  How is the loss function calculated in Faster RCNN?**\n",
        "\n",
        "Faster R-CNN uses a multi-task loss function that combines several components to address different aspects of object detection:\n",
        "\n",
        "1. Region Proposal Network (RPN) Loss:\n",
        "\n",
        "Objectness Loss: This component measures the accuracy of the RPN in predicting whether an anchor box contains an object or not. It's typically implemented using binary cross-entropy loss.\n",
        "Bounding Box Regression Loss: This component measures the difference between the predicted bounding box offsets and the ground-truth bounding box coordinates for positive anchors (those containing objects). It's often implemented using smooth L1 loss.\n",
        "2. Classification and Bounding Box Regression Loss (for RoI Heads):\n",
        "\n",
        "Classification Loss: This component measures the accuracy of the classifier in predicting the class of the object within a region of interest (RoI). It's usually implemented using cross-entropy loss.\n",
        "Bounding Box Regression Loss: This component measures the difference between the predicted bounding box coordinates and the ground-truth bounding box coordinates for the RoI. It's similar to the RPN's bounding box regression loss and is often implemented using smooth L1 loss.\n",
        "Overall Loss Function:\n",
        "\n",
        "The overall loss function in Faster R-CNN is a weighted sum of these individual components:\n",
        "Total Loss = RPN_Loss + RoI_Loss\n",
        "          = (RPN_cls_loss + RPN_bbox_loss) + (RoI_cls_loss + RoI_bbox_loss)\n",
        "\n",
        "**15. Explain how YOLOv9 improves speed compared to earlier versions.**\n",
        "\n",
        "YOLOv9 achieves speed improvements through a combination of architectural optimizations.\n",
        "\n",
        "**16.  What are some challenges faced in training YOLOv9?**\n",
        "\n",
        "While YOLOv9 offers impressive performance, training it effectively can present certain challenges:\n",
        "\n",
        "Data Requirements: YOLOv9, like other deep learning models, benefits from large and diverse datasets. Gathering and annotating a sufficient amount of high-quality training data can be time-consuming and resource-intensive.\n",
        "\n",
        "Computational Resources: Training YOLOv9 requires significant computational power, especially for large datasets or complex configurations. Access to powerful GPUs or cloud computing resources is often necessary for efficient training.\n",
        "\n",
        "Hyperparameter Tuning: Finding the optimal hyperparameters for YOLOv9, such as learning rate, batch size, and data augmentation settings, can be challenging and require experimentation. Careful tuning is crucial for achieving the desired performance.\n",
        "\n",
        "Overfitting: Deep learning models like YOLOv9 are prone to overfitting, especially when trained on limited data. Regularization techniques and strategies like early stopping are essential to prevent overfitting and ensure generalization to unseen data.\n",
        "\n",
        "Class Imbalance: If the dataset has a significant imbalance in the number of instances per class, it can affect the model's ability to detect under-represented classes. Techniques like data augmentation and weighted loss functions can help address this issue.\n",
        "\n",
        "Small Object Detection: YOLOv9, despite its advancements, can still face challenges in detecting small objects, especially those with low resolution or occlusions. Fine-tuning the model and employing strategies like data augmentation that focus on small objects can improve performance in this area.\n",
        "\n",
        "Deployment Considerations: Deploying YOLOv9 for real-time applications may require optimizing the model's size and inference speed. Techniques like model quantization and pruning can help reduce the model's footprint and improve performance on resource-constrained devices.\n",
        "\n",
        "Data Quality: Ensuring the quality of training data is crucial for YOLOv9's success. Inaccurate or inconsistent annotations can negatively impact the model's performance. Thorough data cleaning and verification are essential.\n",
        "\n",
        "**17. How does the YOLOv9 architecture handle large and small object detection?**\n",
        "\n",
        " YOLOv9's architecture tackles large and small object detection by combining multi-scale feature representation, anchor point diversity, and focused attention mechanisms. This holistic approach enables the model to achieve robust object detection performance across a wide range of object sizes.\n",
        "\n",
        " **18.  What is the significance of fine-tuning in YOLO?**\n",
        "\n",
        " Fine-tuning is a crucial technique in object detection, especially when using models like YOLO, to adapt a pre-trained model to a specific dataset or task. It involves taking a model that has been trained on a large dataset (often a general-purpose dataset like COCO) and further training it on a smaller, target dataset.\n",
        "\n",
        " **19.  What is the concept of bounding box regression in Faster RCNN?**\n",
        "\n",
        "The concept of bounding box regression in Faster R-CNN:\n",
        "\n",
        "Bounding box regression is a crucial component of Faster R-CNN (and many other object detection models) that aims to refine the location and size of the predicted bounding boxes to better align with the ground-truth bounding boxes of objects in an image.\n",
        "\n",
        "**20.  Describe how transfer learning is used in YOLO?**\n",
        "\n",
        "Transfer learning is a powerful technique in deep learning where knowledge gained from training a model on a large dataset is applied to a different but related task or dataset. It's widely used in YOLO to improve performance and reduce training time.\n",
        "\n",
        "**21. What is the role of the backbone network in object detection models like YOLOv9?**\n",
        "\n",
        "The backbone network is a fundamental component of object detection models, including YOLOv9. It serves as a feature extractor, responsible for processing the input image and generating a rich set of features that are used for subsequent object detection tasks.\n",
        "\n",
        "**22.  How does YOLO handle overlapping objects?**\n",
        "\n",
        "YOLO (You Only Look Once) employs several strategies to address the challenge of detecting overlapping objects in an image:\n",
        "\n",
        "Anchor Boxes: YOLO uses anchor boxes, which are pre-defined boxes of various shapes and sizes, to predict bounding boxes for objects. By having multiple anchor boxes at each grid cell, YOLO can better capture objects with different aspect ratios and scales, even if they overlap.\n",
        "\n",
        "Non-Maximum Suppression (NMS): After generating initial bounding box predictions, YOLO applies NMS to filter out redundant detections. NMS works by selecting the bounding box with the highest confidence score and suppressing other boxes that significantly overlap with it and have lower scores. This helps to ensure that only one bounding box is predicted for each object, even if multiple boxes initially overlap.\n",
        "\n",
        "Loss Function: The loss function used in YOLO training encourages the model to predict bounding boxes that tightly enclose objects while minimizing overlap with other objects. This helps the model learn to differentiate between overlapping objects and assign them separate bounding boxes.\n",
        "\n",
        "Data Augmentation: During training, data augmentation techniques like random cropping and flipping can be used to introduce variations in object positions and overlaps. This helps the model generalize better to different scenarios and improve its ability to handle overlapping objects.\n",
        "\n",
        "Multi-Scale Feature Fusion: YOLO architectures often incorporate feature pyramid networks (FPNs) or similar mechanisms to fuse features from different scales. This allows the model to detect objects at various sizes, including those that might be partially occluded or overlapping with larger objects.\n",
        "\n",
        "**23. What is the importance of data augmentation in object detection?**\n",
        "\n",
        "Okay, let's discuss the importance of data augmentation in object detection:\n",
        "\n",
        "Data augmentation plays a crucial role in enhancing the performance and robustness of object detection models. It involves applying various transformations to the training data to artificially increase its size and diversity. This helps the model learn to generalize better and perform well on unseen data.\n",
        "\n",
        "**24.  How is performance evaluated in YOLO-based object detection?**\n",
        "\n",
        "Performance evaluation in YOLO-based object detection involves using a combination of metrics like mAP, IoU, precision, recall, F1-score, FPS, and inference time to assess the accuracy, speed, and efficiency of the model in detecting and classifying objects. These metrics provide valuable insights for model selection, comparison, and improvement.\n",
        "\n",
        "**25.  How do the computational requirements of Faster RCNN compare to those of YOLO?**\n",
        "\n",
        "Computational Complexity: Faster R-CNN generally has higher computational complexity compared to YOLO due to its two-stage architecture and the need for region proposal generation.\n",
        "Inference Speed: YOLO typically achieves faster inference speeds than Faster R-CNN due to its more efficient design and single-stage processing.\n",
        "Memory Requirements: Faster R-CNN may require more memory due to the storage of region proposals and intermediate features.\n",
        "\n",
        "**26.  What role do convolutional layers play in object detection with RCNN?**\n",
        "\n",
        "Convolutional layers in RCNN serve as the foundation for object detection by:\n",
        "\n",
        "Extracting meaningful features from images.\n",
        "Creating a hierarchical representation of visual information.\n",
        "Encoding region proposals for object classification and localization.\n",
        "Significance:\n",
        "\n",
        "Feature Learning: Convolutional layers automatically learn relevant features from the data, eliminating the need for manual feature engineering.\n",
        "Translation Invariance: They can detect features regardless of their location in the image, making the model robust to object shifts and variations.\n",
        "Hierarchical Representation: The hierarchical nature of convolutional layers allows the model to capture both low-level and high-level features, enabling it to detect objects of varying complexity.\n",
        "\n",
        "**27.  How does the loss function in YOLO differ from other object detection models?**\n",
        "\n",
        "YOLO (You Only Look Once) uses a unique loss function that is designed to optimize the model for both object detection and classification in a single stage. This differs from other object detection models, such as Faster R-CNN, which employ separate loss functions for region proposal and object classification.\n",
        "\n",
        "**28.  What are the key advantages of using YOLO for real-time object detection?**\n",
        "\n",
        "The key advantages of using YOLO for real-time object detection include its high speed, end-to-end training, ability to incorporate global context, adaptability to different hardware, and open-source nature. These factors make YOLO a powerful and versatile tool for a variety of applications where real-time performance is crucial.\n",
        "\n",
        "**29.  How does Faster RCNN handle the trade-off between accuracy and speed?**\n",
        "\n",
        " Faster R-CNN manages the accuracy-speed trade-off by incorporating efficient region proposal generation, shared convolutional features, RoI pooling, feature caching, and hardware acceleration. While prioritizing accuracy, it achieves a reasonable balance with speed, making it a versatile object detection framework for various applications.\n",
        "\n",
        " **30. What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ?**\n",
        "\n",
        " YOLO's backbone is tailored for real-time performance, balancing speed and accuracy.\n",
        "Faster R-CNN's backbone focuses on extracting rich features for high accuracy, potentially at the cost of slightly slower inference.\n",
        "The choice of backbone network influences the overall performance characteristics of the object detection model, and it's selected based on the specific application requirements and the desired trade-off between accuracy and speed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ABda1zmOjL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "KWpRKbTqYU1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOv9)?**"
      ],
      "metadata": {
        "id": "syvHD0_DYfB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the necessary libraries\n",
        "\n",
        "!pip install utralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pretrained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "\n",
        "# Load your custom images\n",
        "import cv2\n",
        "\n",
        "image_path = 'path/to/your/image.jpg'  # Replace with your image path\n",
        "image = cv2.imread(image_path)"
      ],
      "metadata": {
        "id": "rL2JhYZgYeIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **2. How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture?**"
      ],
      "metadata": {
        "id": "GG1Vv1NoZD-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries:\n",
        "!pip install torchvision==0.15.1 torchaudio==0.15.1 torchtext==0.16.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "#  Import the necessary modules:\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "#  Load the pre-trained ResNet50 backbone:\n",
        "backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "\n",
        "#  Create the Faster R-CNN model with the ResNet50 backbone\n",
        "model = fasterrcnn_resnet50_fpn(backbone=backbone, num_classes=91)  # Assuming 91 classes (COCO dataset)\n",
        "                                                                     # Adjust for your specific use case\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0KiYeyYIZLfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  How do you perform inference on an online image using the Faster RCNN model and print the predictions?**"
      ],
      "metadata": {
        "id": "4JmGyXa-ZlQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries:\n",
        "!pip install torchvision==0.15.1 torchaudio==0.15.1 torchtext==0.16.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Import necessary modules:\n",
        "import torchvision\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "#  Load a pre-trained Faster R-CNN model:\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#  Define a function to preprocess the image:\n",
        "def preprocess_image(image_url):\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    image_tensor = torchvision.transforms.functional.to_tensor(image)\n",
        "    return image_tensor\n",
        "\n",
        "image_url = \"your_image_url\"  # Replace with the URL of the online image\n",
        "image_tensor = preprocess_image(image_url)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "18xvsZkpZ-RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  How do you load an image and perform inference using YOLOv9, then display the detected objects with\n",
        "bounding boxes and class labels?**"
      ],
      "metadata": {
        "id": "A8BnGdFNaVNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # Load a pretrained YOLOv8n model\n",
        "\n",
        "img = cv2.imread(\"path/to/your/image.jpg\")  # Replace with the path to your image\n",
        "\n",
        "results = model(img)  # Perform inference\n",
        "\n",
        "annotated_frame = results[0].plot()  # Plot the detections on the image\n",
        "\n",
        "cv2.imshow(\"YOLOv8 Inference\", annotated_frame)  # Display the annotated image\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "tr83eIFxaawl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  How do you display bounding boxes for the detected objects in an image using Faster RCNN?**"
      ],
      "metadata": {
        "id": "PdynHR30b6vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "image_path = \"path/to/your/image.jpg\"  # Replace with your image path\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image_tensor = F.to_tensor(image)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "\n",
        "# Get the predicted boxes, labels, and scores\n",
        "boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "labels = predictions[0]['labels'].cpu().numpy()\n",
        "scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "# Filter detections based on a confidence threshold (e.g., 0.5)\n",
        "filtered_indices = scores > 0.5\n",
        "boxes = boxes[filtered_indices]\n",
        "labels = labels[filtered_indices]\n",
        "\n",
        "# Convert the image tensor to a NumPy array for display\n",
        "image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "# Draw bounding boxes and labels on the image\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image_np)\n",
        "\n",
        "for box, label in zip(boxes, labels):\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1, f\"{model.classes[label]}\", color='white', backgroundcolor='black')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9z8gE56hcFFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.  How do you perform inference on a local image using Faster RCNN?**"
      ],
      "metadata": {
        "id": "cVcE_0JKcRSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "image_path = \"path/to/your/image.jpg\"  # Replace with the path to your local image\n",
        "image = read_image(image_path)\n",
        "image = F.to_pil_image(image) # Convert to PIL Image\n",
        "image_tensor = F.to_tensor(image)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "\n",
        "# Get the predicted boxes, labels, and scores\n",
        "boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "labels = predictions[0]['labels'].cpu().numpy()\n",
        "scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "# Filter detections based on a confidence threshold (e.g., 0.5)\n",
        "filtered_indices = scores > 0.5\n",
        "boxes = boxes[filtered_indices]\n",
        "labels = labels[filtered_indices]\n",
        "\n",
        "# Convert the image tensor to a NumPy array for display\n",
        "image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "# Draw bounding boxes and labels on the image\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image_np)\n",
        "\n",
        "for box, label in zip(boxes, labels):\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1, f\"{model.classes[label]}\", color='white', backgroundcolor='black')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o4QVEkkEcYiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.  How can you change the confidence threshold for YOLO object detection and filter out low-confidence\n",
        "predictions?**"
      ],
      "metadata": {
        "id": "hefK2TgXcq6c"
      }
    },
    {
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # Load your YOLO model\n",
        "results = model('path/to/your/image.jpg')  # Perform inference\n",
        "\n",
        "# Access the confidence scores for each detection\n",
        "confidence_scores = results[0].boxes.conf.cpu().numpy()\n",
        "\n",
        "# Filter detections based on a threshold (e.g., 0.5)\n",
        "filtered_indices = confidence_scores >= 0.5\n",
        "filtered_results = results[0].boxes[filtered_indices]\n",
        "\n",
        "# Access the filtered bounding boxes, labels, etc.\n",
        "filtered_boxes = filtered_results.xyxy.cpu().numpy()  # Bounding box coordinates\n",
        "filtered_labels = filtered_results.cls.cpu().numpy()  # Class labels"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XjvmKGyFc43w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. How do you plot the training and validation loss curves for model evaluation?**"
      ],
      "metadata": {
        "id": "gZ5UXJCSc6OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using a hypothetical training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ... training logic ...\n",
        "    train_loss = calculate_loss(model, train_data)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # ... validation logic ...\n",
        "    val_loss = calculate_loss(model, val_data)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch/Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WtTfcz5Vc_w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.  How do you perform inference on multiple images from a local folder using Faster RCNN and display the\n",
        "bounding boxes for each?**"
      ],
      "metadata": {
        "id": "eCnDt0m6dQux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "def process_image(image_path):\n",
        "    image = read_image(image_path)\n",
        "    image = F.to_pil_image(image)\n",
        "    image_tensor = F.to_tensor(image)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model([image_tensor])\n",
        "\n",
        "    # Get the predicted boxes, labels, and scores\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "    scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "    # Filter detections based on a confidence threshold (e.g., 0.5)\n",
        "    filtered_indices = scores > 0.5\n",
        "    boxes = boxes[filtered_indices]\n",
        "    labels = labels[filtered_indices]\n",
        "\n",
        "    # Convert the image tensor to a NumPy array for display\n",
        "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Draw bounding boxes and labels on the image\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1, f\"{model.classes[label]}\", color='white', backgroundcolor='black')\n",
        "\n",
        "    plt.title(f\"Detections for {os.path.basename(image_path)}\")\n",
        "    plt.show()\n",
        "\n",
        "image_folder = \"path/to/your/image/folder\"  # Replace with the path to your image folder\n",
        "\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):  # Adjust file extensions as needed\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        process_image(image_path)\n"
      ],
      "metadata": {
        "id": "qQqP33HcdT2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.  How do you visualize the confidence scores alongside the bounding boxes for detected objects using Faster\n",
        "RCNN?**"
      ],
      "metadata": {
        "id": "6utwsBt_dlqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "image_path = \"path/to/your/image.jpg\"  # Replace with your image path\n",
        "image = read_image(image_path)\n",
        "image = F.to_pil_image(image)\n",
        "image_tensor = F.to_tensor(image)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "\n",
        "# Get the predicted boxes, labels, and scores\n",
        "boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "labels = predictions[0]['labels'].cpu().numpy()\n",
        "scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "# Filter detections based on a confidence threshold (e.g., 0.5)\n",
        "filtered_indices = scores > 0.5\n",
        "boxes = boxes[filtered_indices]\n",
        "labels = labels[filtered_indices]\n",
        "scores = scores[filtered_indices]  # Keep scores for filtered detections\n",
        "\n",
        "# Convert the image tensor to a NumPy array for display\n",
        "image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "# Draw bounding boxes, labels, and confidence scores on the image\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image_np)\n",
        "\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1, f\"{model.classes[label]}: {score:.2f}\", color='white', backgroundcolor='black')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3xpvghgFdp7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.  How can you save the inference results (with bounding boxes) as a new image after performing detection\n",
        "using YOLO?**"
      ],
      "metadata": {
        "id": "ZPLAQCL4d5Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # Load a pretrained YOLOv8n model\n",
        "\n",
        "results = model('path/to/your/image.jpg', save=True)  # Perform inference and save results\n",
        "\n",
        "results = model('path/to/your/image.jpg', save=True, project='custom_results', name='my_detection', exist_ok=True)\n",
        "\n",
        "saved_image_path = results[0].save_dir  # Get the save directory path\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aSU4i1wZd8V2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}