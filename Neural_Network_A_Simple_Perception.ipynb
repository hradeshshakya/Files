{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.  What is deep learning, and how is it connected to artificial intelligence.**\n",
        "\n",
        "Ans. Deep learning is a subset of machine learning that uses multilayered neural networks,\n",
        " called deep neural networks, to simulate the complex decision-making power of the human brain.\n",
        " Deep learning systems are made up of neural networks, and the terms are often used interchangeably.\n",
        "  However, there are many different types of neural networks, and not all of them are used in deep learning systems.\n",
        "  Applications\n",
        "\n",
        "Deep learning has many applications in AI, including:\n",
        "Fraud detection\n",
        "Customer service\n",
        "Financial services\n",
        "Natural language processing\n",
        "Facial recognition\n",
        "Self-driving vehicles\n",
        "Predictive analytics\n",
        "Recommender systems"
      ],
      "metadata": {
        "id": "PgJiirKMWX7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **2. What is a neural network, and what are the different types of neural networks?**\n",
        "\n",
        " Ans.  A neural network is a collection of interconnected units called neurons that can perform complex tasks by sending signals to each other.\n",
        "\n",
        " Neural networks are inspired by the human brain and are used in deep learning to allow computers to learn and improve from experience. There are many different types of neural networks, including:\n",
        "\n",
        " Convolutional neural networks (CNNs)\n",
        "These networks are used to process grid-like data, such as images. CNNs use layers of convolutions to apply filters to the input and create feature maps.\n",
        "\n",
        "Recurrent neural networks (RNNs)\n",
        "These networks allow data to flow in any direction and are used in natural language processing (NLP) and language modeling. RNNs can map one-to-one, one-to-many, many-to-one, and many-to-many.\n",
        "Feedforward neural networks\n",
        "These networks have an input layer, hidden layers, and an output layer. Data flows in the forward direction, and there is no backpropagation. They are used for classification, speech recognition, face recognition, and pattern recognition.\n",
        "\n",
        "Multilayer perceptrons (MLPs)\n",
        "These networks have three or more layers and use a nonlinear activation function to classify data.\n",
        "\n",
        "Modular neural networks\n",
        "These networks are made up of a series of independent neural networks that are overseen by an intermediary.\n",
        "\n",
        "Radial basis function neural networks (RBFNNs)\n",
        "These networks are structured with an input layer, a hidden layer, and an output layer. They are used for function approximation.\n",
        "Generative adversarial networks\n",
        "These networks are made up of two neural networks, a generator and a discriminator, that compete against each other. The generator creates fake data, and the discriminator evaluates its authenticity."
      ],
      "metadata": {
        "id": "N53CvDoYVbi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **3. What is the mathematical structure of a neural network?**\n",
        "\n",
        " Ans.  Neural networks are algorithms, which compute, from an input x (for example an image), an\n",
        " output y.\n",
        "\n",
        "  Mathematically, such an algorithm defines a function fw (i.e. y = fw(x))."
      ],
      "metadata": {
        "id": "r00ze73yzcOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **4. What is an activation function, and why is it essential in neural?**\n",
        "\n",
        " Ans. Activation functions are an integral building block of neural networks that enable them to learn complex patterns in data. They transform the input signal of a node in a neural network into an output signal that is then passed on to the next layer."
      ],
      "metadata": {
        "id": "CLWp9mt60KV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Could you list some common activation functions used in neural networks.**\n",
        "\n",
        "Ans.\n",
        "\n",
        "1. Sigmoid Function\n",
        "2. Tanh Activation Function\n",
        "3. ReLU (Rectified Linear Unit) Function\n",
        "4. Linear Activation Function\n",
        "5. Softmax Function\n"
      ],
      "metadata": {
        "id": "Ue2eq_KU0dsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is a multilayer neural network?**\n",
        "\n",
        "Ans.A multilayer neural network is a machine learning algorithm that uses multiple layers of interconnected neurons to learn complex patterns and relationships in data. It's also known as an artificial neural network (ANN) or deep neural network (DNN)"
      ],
      "metadata": {
        "id": "ZyLFhcE51Mg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **7. What is a loss function, and why is it crucial for neural network training?**\n",
        "\n",
        " A loss function is a mathematical function that measures how well a neural network model performs a task, and is a vital component of neural network training:\n",
        "How it works\n",
        "A loss function calculates the difference between a model's predictions and the actual values in a dataset. The loss function outputs a higher number when predictions are incorrect, and a lower number when predictions are closer to the actual values.\n",
        "Why it's important\n",
        "A loss function is crucial for neural network training because it helps guide the model training process towards making correct predictions. During training, a learning algorithm uses the loss function to adjust the model's parameters and minimize the loss. This improves the model's performance on the dataset.\n",
        "Different types of loss functions\n",
        "Different loss functions are used for different tasks and problems. For example, the mean squared error (MSE) loss function is used for regression tasks, while the cross-entropy loss function is used for classification tasks."
      ],
      "metadata": {
        "id": "UeUbTP9U1bc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What are some common types of loss functions?**\n",
        "\n",
        "Ans. Some common loss functions include:\n",
        "\n",
        "Mean squared error (MSE): A basic loss function that's often used for regression problems. It measures the average squared difference between the predicted and true values.\n",
        "\n",
        "Log loss (cross entropy loss): A popular loss function for classification problems that penalizes being very confident and very wrong.\n",
        "Huber loss: A combination of MSE and MAE that's less sensitive to outliers. It's also called smooth L1 loss.\n",
        "\n",
        "Log-cosh loss: A smoother approximation to the absolute difference between predicted and true values. It's useful for financial forecasting and anomaly detection.\n",
        "\n",
        "Mean absolute error (MAE): Measures the absolute difference between the predicted and true values.\n",
        "\n",
        "Kullback Leibler Divergence Loss (KL Loss): Gauges how different a distribution is from a standard distribution.\n",
        "\n",
        "Other loss functions include: Likelihood loss, Hinge loss, Categorical Cross-Entropy Loss, and Quantile Loss."
      ],
      "metadata": {
        "id": "ddP-XJD71sgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. How does a neural network learn?**\n",
        "Ans. Neural networks learn by adjusting the weights of connections between neurons through a process called training. The training process involves:\n",
        "Feeding the network data: The network is fed large sets of labeled or unlabeled data.\n",
        "\n",
        "Calculating the loss function: The loss function calculates the error between the predicted and actual values. The goal is to minimize the loss function's output.\n",
        "\n",
        "Adjusting weights and biases: The network adjusts weights and biases to minimize the difference between the actual and correct results. This process is called backpropagation."
      ],
      "metadata": {
        "id": "eWasnapu2D7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What is an optimizer in neural networks, and why is it necessary?**\n",
        "\n",
        "Ans. An optimizer in a neural network is an algorithm that updates the weights and other attributes of a neural network during training to minimize the loss function:\n",
        "\n",
        "What it does\n",
        "Optimizers help find the best set of weights to minimize the loss function, which measures the difference between the predicted and actual values of the target variable.\n",
        "\n",
        "Why it's necessary\n",
        "Optimizers are essential for improving the performance of a neural network model. The choice of optimizer can significantly impact the efficiency, convergence speed, and overall performance of the model.\n",
        "\n",
        "How it works\n",
        "Optimizers use different strategies to converge towards optimal parameter values. For example, SGD with Momentum adds a momentum term to regular stochastic gradient descent. This simulates the inertia of an object when it is moving, which can help increase stability and learn faster.\n",
        "\n",
        "Other considerations\n",
        "Optimizers can also handle other important aspects of training, such as regularization techniques. Regularization helps prevent overfitting, which occurs when a neural network performs well on training data but fails to generalize to unseen data."
      ],
      "metadata": {
        "id": "u-v_HtJC2cdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Could you briefly describe some common optimizers?**\n",
        "\n",
        "1 - Stochastic Gradient descent\n",
        "Stochastic Gradient Descent (SGD) is an iterative optimization algorithm commonly used in machine learning and deep learning. It is a variant of gradient descent that performs updates to the model parameters (weights) based on the gradient of the loss function computed on a randomly selected subset of the training data, rather than on the full dataset.\n",
        "\n",
        "2 - Adagrad\n",
        "Adagrad (Adaptive Gradient) is an optimization algorithm used in machine learning and deep learning to optimize the training of neural networks.\n",
        "\n",
        "The Adagrad algorithm adjusts the learning rate of each parameter of the neural network adaptively during the training process. Specifically, it scales the learning rate of each parameter based on the historical gradients computed for that parameter. In other words, parameters that have large gradients are given a smaller learning rate, while those with small gradients are given a larger learning rate. This helps prevent the learning rate from decreasing too quickly for frequently occurring parameters and allows for faster convergence of the training process.\n",
        "\n",
        "3 - RMSProp\n",
        "RMSProp (Root Mean Square Propagation) is an optimization algorithm used in machine learning and deep learning to optimize the training of neural networks.\n",
        "\n",
        "Like Adagrad and Adadelta, RMSProp adapts the learning rate of each parameter during the training process. However, instead of accumulating all the past gradients like Adagrad, RMSProp computes a moving average of the squared gradients. This allows the algorithm to adjust the learning rate more smoothly, and it prevents the learning rate from decreasing too quickly.\n",
        "\n",
        "4 - Adam\n",
        "Adam (Adaptive Moment Estimation) is an optimization algorithm used in machine learning and deep learning to optimize the training of neural networks.\n",
        "\n",
        "Adam combines the concepts of both momentum and RMSProp. It maintains a moving average of the gradient's first and second moments, which are the mean and variance of the gradients, respectively. The moving average of the first moment, which is similar to the momentum term in other optimization algorithms, helps the optimizer to continue moving in the same direction even when the gradients become smaller. The moving average of the second moment, which is similar to the RMSProp term, helps the optimizer to scale the learning rate for each parameter based on the variance of the gradients."
      ],
      "metadata": {
        "id": "whg9AOeR24bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Can you explain forward and backward propagation in a neural network?**\n",
        "\n",
        "Forward propagation and backward propagation are two processes that work together to train a neural network:\n",
        "\n",
        "Forward propagation\n",
        "Moves data from the input layer to the output layer of a neural network. The neural network calculates the output after the input is fed in.\n",
        "\n",
        "Backward propagation\n",
        "Moves from the output layer to the input layer, calculating the error between the predicted and actual output. The weights and biases of each neuron are then adjusted to reduce the error.\n"
      ],
      "metadata": {
        "id": "xYcNu5nk35Ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. What is weight initialization, and how does it impact training?**\n",
        "\n",
        "Weight initialization is a critical step in building neural networks because it sets the starting point for the model's optimization process. Poorly initialized weights can lead to slow convergence, vanishing or exploding gradients, and hinder the overall learning process.\n"
      ],
      "metadata": {
        "id": "ev12HUle4Qtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. What is the vanishing gradient problem in deep learning?**\n",
        "\n",
        "The vanishing gradient problem is a challenge in deep learning that occurs when gradients used to update neural network weights become very small during backpropagation:"
      ],
      "metadata": {
        "id": "heomvrpm4e53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. What is the exploding gradient problem?**\n",
        "\n",
        "The exploding gradient problem is a challenge that occurs during the training of deep neural networks. It happens when the gradients of the network's loss with respect to the parameters (weights) become too large. This can lead to numerical instability and the network being unable to converge to a suitable solution."
      ],
      "metadata": {
        "id": "kYmtM9F14zeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "uIBa9G0I6CfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How do you create a simple perceptron for basic binary classification**"
      ],
      "metadata": {
        "id": "zl8rLfQR5kHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Create a Perceptron model\n",
        "clf = Perceptron(max_iter=1000, eta0=0.1)"
      ],
      "metadata": {
        "id": "ACAi0rSK0Jey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. How can you build a neural network with one hidden layer using Keras?**"
      ],
      "metadata": {
        "id": "79bK3fp751sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependencies\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# Neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=20, activation='relu'))\n",
        "model.add(Dense(1, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRj7gHTX6MNF",
        "outputId": "fe34170e-aa84-47ee-acca-d36a4835646f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras.**\n",
        "\n"
      ],
      "metadata": {
        "id": "v4gbyJs068a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple neural network with Xavier Initialization\n",
        "model = models.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Hidden layers with Xavier Initialization\n",
        "model.add(layers.Dense(128, kernel_initializer='glorot_uniform', activation='relu'))\n",
        "model.add(layers.Dense(64, kernel_initializer='glorot_uniform', activation='relu'))\n",
        "\n",
        "# Output layer with 10 units (for 10 classes) and softmax activation\n",
        "model.add(layers.Dense(\n",
        "    10, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QrZuFBn56t0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. How can you apply different activation functions in a neural network in Keras**"
      ],
      "metadata": {
        "id": "KoU7cuMf7Rs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependencies\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# Neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=20, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))"
      ],
      "metadata": {
        "id": "IIDpST6p7bFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  How do you add dropout to a neural network model to prevent overfitting.**"
      ],
      "metadata": {
        "id": "IhxSG2-r8Ncx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(16, input_dim=5, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "3T8cC8Ra8ZUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. How do you manually implement forward propagation in a simple neural network**"
      ],
      "metadata": {
        "id": "fywhCqlW8t5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parameters Initialization. We will first initialize the weight matrices and the bias vectors. ...\n",
        "# Step 2: Activation function. ...\n",
        "# Step 3: Feeding forward.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Creating a dataset\n",
        "df = pd.DataFrame([[8, 8, 4], [7, 9, 5], [6, 10, 6], [5, 12, 7]], columns=['cgpa', 'profile_score', 'lpa'])\n",
        "\n",
        "# Initializing parameters\n",
        "def initialize_parameters(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "    for i in range(1, L):\n",
        "        parameters['W' + str(i)] = np.ones((layer_dims[i-1], layer_dims[i])) * 0.1\n",
        "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
        "    return parameters\n",
        "\n",
        "# Forward propagation\n",
        "def linear_forward(A_prev, W, b):\n",
        "    Z = np.dot(W.T, A_prev) + b\n",
        "    return Z\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def L_layer_forward(X, parameters):\n",
        "    A = X\n",
        "    caches = []\n",
        "    L = len(parameters) // 2\n",
        "    for i in range(1, L):\n",
        "        A_prev = A\n",
        "        W = parameters['W' + str(i)]\n",
        "        b = parameters['b' + str(i)]\n",
        "        Z = linear_forward(A_prev, W, b)\n",
        "        A = relu(Z)\n",
        "        cache = (A_prev, W, b, Z)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Output layer\n",
        "    W_out = parameters['W' + str(L)]\n",
        "    b_out = parameters['b' + str(L)]\n",
        "    Z_out = linear_forward(A, W_out, b_out)\n",
        "    AL = Z_out\n",
        "\n",
        "    return AL, caches\n",
        "\n",
        "# Example execution\n",
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2, 1)\n",
        "parameters = initialize_parameters([2, 2, 1])\n",
        "y_hat, caches = L_layer_forward(X, parameters)\n",
        "print(\"Final output:\")\n",
        "print(y_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rJ9lok49NC1",
        "outputId": "c11101c7-ef79-4862-854e-bf132019807b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output:\n",
            "[[0.32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. How do you add batch normalization to a neural network model in Keras**"
      ],
      "metadata": {
        "id": "rF5LnI0M8z9N"
      }
    },
    {
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IWQuTTZp9yNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        " from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "\n",
        " model = Sequential()\n",
        " model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
        " model.add(BatchNormalization())  # Add batch normalization after the first dense layer\n",
        " model.add(Dense(10, activation='softmax'))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8WWLycRo901i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. How can you visualize the training process with accuracy and loss curves.**"
      ],
      "metadata": {
        "id": "uwQvXq-393on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow_datasets import load_dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_dataset('mnist')"
      ],
      "metadata": {
        "id": "PYJdHNQK-xU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "7RvC5bzF-Q0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gKDaoMdU-FFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hgE-d-Ii-GNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?**"
      ],
      "metadata": {
        "id": "wMChPtXaAff9"
      }
    },
    {
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Define the optimizer with gradient clipping\n",
        "optimizer = keras.optimizers.Adam(clipvalue=1.0)\n",
        "\n",
        "# Compile the model with the optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IBFl1MgHAyL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **10. How can you create a custom loss function in Keras?**"
      ],
      "metadata": {
        "id": "wPhxI13eA3Ph"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Calculate the squared difference between true and predicted values\n",
        "    squared_difference = tf.square(y_true - y_pred)\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    loss = tf.reduce_mean(squared_difference, axis=-1)\n",
        "\n",
        "    return loss"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TbAuPlX-BEtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **11. How can you visualize the structure of a neural network model in Keras?**"
      ],
      "metadata": {
        "id": "9G7xf6wWBHpH"
      }
    },
    {
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assuming 'model' is your Keras model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-5x69sCqBSUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}